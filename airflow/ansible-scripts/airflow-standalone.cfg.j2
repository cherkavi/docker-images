[core]
dags_folder = /home/ubuntu/airflow/dags

hostname_callable = socket.getfqdn

default_timezone = utc

executor = LocalExecutor

sql_alchemy_conn = postgresql+psycopg2://{{ lookup('env','AIRFLOW_DB_USERNAME') }}:{{ lookup('env','AIRFLOW_DB_PASSWORD') }}@{{ lookup('env','AIRFLOW_DB_HOST') }}:{{ lookup('env','AIRFLOW_DB_PORT') }}/{{ lookup('env','AIRFLOW_DB_DATABASE') }}

sql_engine_encoding = utf-8

sql_alchemy_pool_enabled = True

sql_alchemy_pool_size = 5

sql_alchemy_max_overflow = 10

sql_alchemy_pool_recycle = 1800

sql_alchemy_pool_pre_ping = True

sql_alchemy_schema =

parallelism = 32

dag_concurrency = 16

dags_are_paused_at_creation = True

max_active_runs_per_dag = 16

load_examples = False

load_default_connections = True

plugins_folder = /home/ubuntu/airflow/plugins

execute_tasks_new_python_interpreter = False

fernet_key = 

donot_pickle = True

dagbag_import_timeout = 30.0

dagbag_import_error_tracebacks = True

dagbag_import_error_traceback_depth = 2

dag_file_processor_timeout = 50

task_runner = StandardTaskRunner

default_impersonation =

security =

unit_test_mode = False

enable_xcom_pickling = False

killed_task_cleanup_time = 60

dag_run_conf_overrides_params = True

dag_discovery_safe_mode = True

default_task_retries = 0

min_serialized_dag_update_interval = 30

min_serialized_dag_fetch_interval = 10

max_num_rendered_ti_fields_per_task = 30

check_slas = True

xcom_backend = airflow.models.xcom.BaseXCom

lazy_load_plugins = True

lazy_discover_providers = True

max_db_retries = 3

[logging]
base_log_folder = /home/ubuntu/airflow/logs

remote_logging = False

remote_log_conn_id =

google_key_path =

remote_base_log_folder =

encrypt_s3_logs = False

logging_level = INFO

fab_logging_level = WARN

logging_config_class =

colored_console_log = True

colored_log_format = {{ '[%%(blue)s%%(asctime)s%%(reset)s] {%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s' }}
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter

log_format = {{ '[%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s' }}
simple_log_format = {{ '%%(asctime)s %%(levelname)s - %%(message)s' }} 

task_log_prefix_template =

log_filename_template = {{ '{{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log' }}

log_processor_filename_template = {{ '{{ filename }}.log' }}

dag_processor_manager_log_location = /home/ubuntu/airflow/logs/dag_processor_manager/dag_processor_manager.log

task_log_reader = task

extra_loggers =

[metrics]

statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

statsd_allow_list =

stat_name_handler =

statsd_datadog_enabled = False

statsd_datadog_tags =


[secrets]
backend =

backend_kwargs =

[cli]
api_client = airflow.api.client.local_client

endpoint_url = http://localhost:{{ lookup('env','AIRFLOW_PORT') }}

[debug]
fail_fast = False

[api]
enable_experimental_api = False

auth_backend = airflow.api.auth.backend.basic_auth

maximum_page_limit = 100

fallback_page_limit = 100

google_oauth2_audience =

google_key_path =

[lineage]
backend =

[atlas]
sasl_enabled = False
host =
port = 21000
username =
password =

[operators]
default_owner = airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0

allow_illegal_arguments = False

[hive]
default_hive_mapred_queue =


[webserver]
base_url = {{ lookup('env','AIRFLOW_URL') }}

default_ui_timezone = UTC

web_server_host = 0.0.0.0

web_server_port = {{ lookup('env','AIRFLOW_PORT') }}

web_server_ssl_cert =

web_server_ssl_key =

web_server_master_timeout = 120

web_server_worker_timeout = 120

worker_refresh_batch_size = 1

worker_refresh_interval = 30

reload_on_plugin_change = False

secret_key = UiuyTOQ4Ajet8qFo5Ct/WA==

workers = 4

worker_class = sync

access_logfile = -

error_logfile = -

access_logformat =

expose_config = False

expose_hostname = True

expose_stacktrace = True

dag_default_view = tree

dag_orientation = LR

demo_mode = False

log_fetch_timeout_sec = 5

log_fetch_delay_sec = 2

log_auto_tailing_offset = 30

log_animation_speed = 1000

hide_paused_dags_by_default = False

page_size = 100

navbar_color = #fff

default_dag_run_display_number = 25

enable_proxy_fix = False

proxy_fix_x_for = 1

proxy_fix_x_proto = 1

proxy_fix_x_host = 1

proxy_fix_x_port = 1

proxy_fix_x_prefix = 1

cookie_secure = False

cookie_samesite = Lax

default_wrap = False

x_frame_enabled = True



show_recent_stats_for_completed_runs = True

update_fab_perms = True

session_lifetime_minutes = 43200

instance_name = {{ lookup('env','AIRFLOW_NAME') }}
[email]

email_backend = airflow.utils.email.send_email_smtp

default_email_on_retry = True

default_email_on_failure = True



[smtp]

smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 25
smtp_mail_from = airflow@example.com
smtp_timeout = 30
smtp_retry_limit = 5

[sentry]

sentry_on = false
sentry_dsn =

[celery_kubernetes_executor]

kubernetes_queue = kubernetes

[celery]

celery_app_name = airflow.executors.celery_executor

worker_concurrency = 16



worker_log_server_port = {{ worker_log_server_port | default("8082") }}

worker_umask = 0o077

broker_url = pyamqp://{{ lookup('env','AIRFLOW_MQ_LOGIN') }}:{{ lookup('env','AIRFLOW_MQ_PASS') }}@{{ lookup('env','AIRFLOW_MQ_URL') }}

result_backend = db+postgresql://{{ lookup('env','DB_VARIANT_USERNAME') }}:{{ lookup('env','DB_VARIANT_PASSWORD') }}@{{ lookup('env','DB_VARIANT_HOST') }}:{{ lookup('env','DB_VARIANT_PORT') }}/{{ lookup('env','DB_VARIANT_DATABASE') }}

flower_host = 

flower_url_prefix =

flower_port = {{ flower_port | default("8081")}} 

flower_basic_auth = {{ lookup('env','AIRFLOW_FLOWER_LOGIN') }}:{{ lookup('env','AIRFLOW_FLOWER_PASSWORD') }}

default_queue = default

sync_parallelism = 0

celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG
ssl_active = False
ssl_key =
ssl_cert =
ssl_cacert =

pool = prefork

operation_timeout = 1.0

task_track_started = True

task_adoption_timeout = 600

task_publish_max_retries = 3

worker_precheck = False

[celery_broker_transport_options]


[dask]

cluster_address = 127.0.0.1:8786

tls_ca =
tls_cert =
tls_key =

[scheduler]
job_heartbeat_sec = 5
clean_tis_without_dagrun_interval = 15.0
scheduler_heartbeat_sec = 5
num_runs = -1
processor_poll_interval = 1
min_file_process_interval = 30
dag_dir_list_interval = 30
print_stats_interval = 30
pool_metrics_interval = 5.0
scheduler_health_check_threshold = 30
orphaned_tasks_check_interval = 300.0
child_process_log_directory = /home/ubuntu/airflow/logs/scheduler
scheduler_zombie_task_threshold = 300
catchup_by_default = False

max_tis_per_query = 64

use_row_level_locking = True
use_job_schedule = True
allow_trigger_in_future = True


[kerberos]
ccache = /tmp/airflow_krb5_ccache

principal = airflow
reinit_frequency = 3600
kinit_path = kinit
keytab = airflow.keytab

[github_enterprise]
api_rev = v3

[admin]
hide_sensitive_variable_fields = True

sensitive_variable_fields =

[elasticsearch]
host =

log_id_template = {dag_id}-{task_id}-{execution_date}-{try_number}

end_of_log_mark = end_of_log

frontend =

write_stdout = False

json_format = False

json_fields = asctime, filename, lineno, levelname, message

[elasticsearch_configs]
use_ssl = False
verify_certs = True

[kubernetes]
pod_template_file =

worker_container_repository =

worker_container_tag =

namespace = default

delete_worker_pods = True

delete_worker_pods_on_failure = False

worker_pods_creation_batch_size = 1

multi_namespace_mode = False

in_cluster = True



kube_client_request_args =

delete_option_kwargs =

enable_tcp_keepalive = True

tcp_keep_idle = 120

tcp_keep_intvl = 30

tcp_keep_cnt = 6

[smart_sensor]
use_smart_sensor = False

shard_code_upper_limit = 10000

shards = 5

sensors_enabled = NamedHivePartitionSensor
